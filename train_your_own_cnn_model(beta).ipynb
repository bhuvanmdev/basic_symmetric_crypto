{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTpNJkZU71QzQqJWbls/Xh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvanmdev/basic_symmetric_crypto/blob/main/train_your_own_cnn_model(beta).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown tick the box below if you want detailed summary of the\n",
        "#@markdown process, else ignore it and run all\n",
        "Detailed_results = True #@param{type:\"boolean\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hGyn3rQoqp94"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Install all required modules in a single go*"
      ],
      "metadata": {
        "id": "a31kSyp1Qcai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe opencv-python tensorflow numpy pandas matplotlib seaborn > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "fJl0PJ8qQb71"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNZIP THE IMAGES**"
      ],
      "metadata": {
        "id": "olt9e83Br991"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown name variable refers to the imported zip name\n",
        "name = \"ar\" #@param {allow-input: true}\n",
        "!unzip -q {name}.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "uFDMfwLmsA8F",
        "outputId": "e52c5f08-c27b-48d4-b6a4-c6d331498387"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "warning:  ar.zip appears to use backslashes as path separators\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORT ALL REQUIRED MODULES**"
      ],
      "metadata": {
        "id": "PAVg23QNQ6Px"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ze2R-fOGP17E"
      },
      "outputs": [],
      "source": [
        "#@title Default title text\n",
        "import mediapipe as mp\n",
        "import cv2 as cv\n",
        "import csv\n",
        "import os\n",
        "import copy,itertools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All the paths of files/images and modules**"
      ],
      "metadata": {
        "id": "1jTjdiyySZAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "model_save_path = 'keypoint_classifier.hdf5'\n",
        "tflite_save_path = 'keypoint_classifier.tflite'\n",
        "csv_path = name+'.csv'\n",
        "image_folder = r\"data\""
      ],
      "metadata": {
        "id": "rYkHkCXKSYdN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converts images-->hand_labels in csv format**"
      ],
      "metadata": {
        "id": "boQgwObrQFfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title .\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "        static_image_mode=True,\n",
        "        max_num_hands=1,\n",
        "        min_detection_confidence=0.7)\n",
        "\n",
        "\n",
        "def calc_landmark_list(image, landmarks):\n",
        "    image_height_y,image_width_x,_  = image.shape\n",
        "\n",
        "    landmark_point = []\n",
        "\n",
        "    # Keypoint\n",
        "    for landmark in landmarks.landmark: # 21/42  loops for learning 1/2 hands\n",
        "        landmark_x = min(int(landmark.x * image_width_x), image_width_x - 1)\n",
        "        landmark_y = min(int(landmark.y * image_height_y), image_height_y - 1)\n",
        "        # landmark_z = landmark.z\n",
        "\n",
        "        landmark_point.append([landmark_x, landmark_y])\n",
        "\n",
        "    return landmark_point\n",
        "\n",
        "def pre_process_landmark(landmark_list):\n",
        "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
        "\n",
        "    # Convert to relative coordinates(having a x=0 and y=0)\n",
        "    base_x, base_y = 0, 0\n",
        "    for index, landmark_point in enumerate(temp_landmark_list):\n",
        "        if index == 0:\n",
        "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
        "\n",
        "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
        "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
        "\n",
        "    # Convert to a one-dimensional list\n",
        "    temp_landmark_list = list(\n",
        "        itertools.chain.from_iterable(temp_landmark_list))\n",
        "\n",
        "    # Normalization\n",
        "    max_value = max(list(map(abs, temp_landmark_list)))\n",
        "\n",
        "    def normalize_(n):\n",
        "        return n / max_value\n",
        "\n",
        "    temp_landmark_list = list(map(normalize_, temp_landmark_list))#convert them from 0-1 range\n",
        "\n",
        "    return temp_landmark_list\n",
        "\n",
        "def logging_csv(number,landmark_list):\n",
        "    with open(csv_path, 'a', newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([number, *landmark_list])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "labels = []\n",
        "number = 0\n",
        "# os.chdir(os.listdir(image_folder)[0])\n",
        "for number,files in enumerate(sorted(os.listdir(image_folder))):\n",
        "    for img in os.listdir(os.path.join(image_folder,files)):\n",
        "        img_path = os.path.join(image_folder,files,img)\n",
        "        image = cv.imread(img_path)\n",
        "        image_bgr = cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
        "        results = hands.process(image_bgr)\n",
        "        if results.multi_hand_landmarks is not None:\n",
        "            for hand_landmarks, handedness in zip(results.multi_hand_landmarks,\n",
        "                                                  results.multi_handedness):\n",
        "\n",
        "                landmark_list = calc_landmark_list(image, hand_landmarks)\n",
        "\n",
        "                # Conversion to relative coordinates / normalized coordinates\n",
        "                pre_processed_landmark_list = pre_process_landmark(\n",
        "                    landmark_list)\n",
        "#\n",
        "                # Write to the dataset file\n",
        "                logging_csv(number, pre_processed_landmark_list)\n",
        "NUM_CLASSES = number+1"
      ],
      "metadata": {
        "id": "uFTukPEhQB-o"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD THE DATASETS FROM THE CSV FILE INTO *test* and *train* set**"
      ],
      "metadata": {
        "id": "wM8b4zIrRZyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_dataset = np.loadtxt(csv_path, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))\n",
        "y_dataset = np.loadtxt(csv_path, delimiter=',', dtype='int32', usecols=(0))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED) # you can change the train_size between 0.75-0.85 for good efficieny"
      ],
      "metadata": {
        "id": "pLIds3f_RpVY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL CONSTRUCTION**"
      ],
      "metadata": {
        "id": "PtT1i82aTycf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    Input((21 * 2,)),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(5, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation='relu'),\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "#100 images per class is a minimum"
      ],
      "metadata": {
        "id": "iRPWmF4IT-p9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL SUMMARY**"
      ],
      "metadata": {
        "id": "G_OOsmOXVkKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if Detailed_results:\n",
        "  model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUHm4ZXfVjFV",
        "outputId": "af31894a-b42a-44c5-f1f6-e2528ecfd2ca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dropout (Dropout)           (None, 42)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                430       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 10)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 55        \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 5)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 18        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 8         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 511\n",
            "Trainable params: 511\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT CALLBACKS**"
      ],
      "metadata": {
        "id": "RXUrYm3tVs9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback to save the progress at intervals of time\n",
        "display(cp_callback := tf.keras.callbacks.ModelCheckpoint(\n",
        "    model_save_path, verbose=1, save_weights_only=False))\n",
        "# Callback for early stopping to avoid overfitting\n",
        "display(es_callback := tf.keras.callbacks.EarlyStopping(patience=20, verbose=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "PlJmus9WVtME",
        "outputId": "b24f9337-58dd-4e97-884e-8fb6e7183f3e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<keras.callbacks.ModelCheckpoint at 0x7f22f2f7f3d0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<keras.callbacks.EarlyStopping at 0x7f22f2f7dcc0>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COMPILE THE MODEL WITH REQUIRED SETTINGS**"
      ],
      "metadata": {
        "id": "IivAZXOFWh6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KrK_flhDWhHg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL TRAINING**"
      ],
      "metadata": {
        "id": "TF8EO3Q7XEHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    #@markdown you can change these values based on the amount of classes present\n",
        "    epochs=30#@param\n",
        "    ,\n",
        "    batch_size=5#@param\n",
        "    ,validation_data=(X_test, y_test),\n",
        "    callbacks=[cp_callback, es_callback]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "YOA8SFdIXDMz",
        "outputId": "21e8190d-de94-47e2-e7e1-ed45ba71092f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "31/41 [=====================>........] - ETA: 0s - loss: 0.5179 - accuracy: 0.7742\n",
            "Epoch 1: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.5167 - accuracy: 0.7610 - val_loss: 0.3833 - val_accuracy: 0.9855\n",
            "Epoch 2/30\n",
            "38/41 [==========================>...] - ETA: 0s - loss: 0.4974 - accuracy: 0.7789\n",
            "Epoch 2: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.4999 - accuracy: 0.7756 - val_loss: 0.3500 - val_accuracy: 0.9855\n",
            "Epoch 3/30\n",
            "35/41 [========================>.....] - ETA: 0s - loss: 0.4745 - accuracy: 0.8171\n",
            "Epoch 3: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 7ms/step - loss: 0.4800 - accuracy: 0.7951 - val_loss: 0.3332 - val_accuracy: 0.9855\n",
            "Epoch 4/30\n",
            "37/41 [==========================>...] - ETA: 0s - loss: 0.4788 - accuracy: 0.8108\n",
            "Epoch 4: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 7ms/step - loss: 0.4792 - accuracy: 0.8146 - val_loss: 0.3355 - val_accuracy: 0.9855\n",
            "Epoch 5/30\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.4485 - accuracy: 0.8400\n",
            "Epoch 5: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 7ms/step - loss: 0.4491 - accuracy: 0.8390 - val_loss: 0.3091 - val_accuracy: 0.9855\n",
            "Epoch 6/30\n",
            "33/41 [=======================>......] - ETA: 0s - loss: 0.4443 - accuracy: 0.8485\n",
            "Epoch 6: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.4550 - accuracy: 0.8244 - val_loss: 0.3066 - val_accuracy: 0.9855\n",
            "Epoch 7/30\n",
            "27/41 [==================>...........] - ETA: 0s - loss: 0.3910 - accuracy: 0.8963\n",
            "Epoch 7: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.4059 - accuracy: 0.8683 - val_loss: 0.2739 - val_accuracy: 1.0000\n",
            "Epoch 8/30\n",
            "39/41 [===========================>..] - ETA: 0s - loss: 0.4195 - accuracy: 0.8308\n",
            "Epoch 8: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.4191 - accuracy: 0.8341 - val_loss: 0.2706 - val_accuracy: 0.9855\n",
            "Epoch 9/30\n",
            "34/41 [=======================>......] - ETA: 0s - loss: 0.3786 - accuracy: 0.9000\n",
            "Epoch 9: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.3818 - accuracy: 0.8927 - val_loss: 0.2493 - val_accuracy: 1.0000\n",
            "Epoch 10/30\n",
            "24/41 [================>.............] - ETA: 0s - loss: 0.3968 - accuracy: 0.8500\n",
            "Epoch 10: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 7ms/step - loss: 0.3816 - accuracy: 0.8585 - val_loss: 0.2306 - val_accuracy: 1.0000\n",
            "Epoch 11/30\n",
            "34/41 [=======================>......] - ETA: 0s - loss: 0.3482 - accuracy: 0.9118\n",
            "Epoch 11: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 7ms/step - loss: 0.3552 - accuracy: 0.8927 - val_loss: 0.2229 - val_accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "33/41 [=======================>......] - ETA: 0s - loss: 0.3255 - accuracy: 0.9091\n",
            "Epoch 12: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.3226 - accuracy: 0.9122 - val_loss: 0.2100 - val_accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.8850\n",
            "Epoch 13: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 1s 18ms/step - loss: 0.3469 - accuracy: 0.8829 - val_loss: 0.2011 - val_accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "36/41 [=========================>....] - ETA: 0s - loss: 0.2733 - accuracy: 0.9444\n",
            "Epoch 14: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 1s 25ms/step - loss: 0.2696 - accuracy: 0.9415 - val_loss: 0.1960 - val_accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "38/41 [==========================>...] - ETA: 0s - loss: 0.3273 - accuracy: 0.8947\n",
            "Epoch 15: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 1s 18ms/step - loss: 0.3269 - accuracy: 0.8976 - val_loss: 0.1855 - val_accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.9050\n",
            "Epoch 16: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.3081 - accuracy: 0.9073 - val_loss: 0.1755 - val_accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "24/41 [================>.............] - ETA: 0s - loss: 0.2809 - accuracy: 0.9167\n",
            "Epoch 17: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.2726 - accuracy: 0.9366 - val_loss: 0.1666 - val_accuracy: 1.0000\n",
            "Epoch 18/30\n",
            "22/41 [===============>..............] - ETA: 0s - loss: 0.3116 - accuracy: 0.9000\n",
            "Epoch 18: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.2827 - accuracy: 0.9220 - val_loss: 0.1620 - val_accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9122\n",
            "Epoch 19: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.2878 - accuracy: 0.9122 - val_loss: 0.1553 - val_accuracy: 1.0000\n",
            "Epoch 20/30\n",
            "23/41 [===============>..............] - ETA: 0s - loss: 0.2503 - accuracy: 0.9478\n",
            "Epoch 20: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.2552 - accuracy: 0.9463 - val_loss: 0.1549 - val_accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "22/41 [===============>..............] - ETA: 0s - loss: 0.2269 - accuracy: 0.9455\n",
            "Epoch 21: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.2523 - accuracy: 0.9415 - val_loss: 0.1464 - val_accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "32/41 [======================>.......] - ETA: 0s - loss: 0.2402 - accuracy: 0.9375\n",
            "Epoch 22: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.2615 - accuracy: 0.9220 - val_loss: 0.1419 - val_accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "32/41 [======================>.......] - ETA: 0s - loss: 0.2426 - accuracy: 0.9438\n",
            "Epoch 23: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.2344 - accuracy: 0.9512 - val_loss: 0.1352 - val_accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "31/41 [=====================>........] - ETA: 0s - loss: 0.3179 - accuracy: 0.9097\n",
            "Epoch 24: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2927 - accuracy: 0.9220 - val_loss: 0.1312 - val_accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "31/41 [=====================>........] - ETA: 0s - loss: 0.2554 - accuracy: 0.9355\n",
            "Epoch 25: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.2544 - accuracy: 0.9366 - val_loss: 0.1340 - val_accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "26/41 [==================>...........] - ETA: 0s - loss: 0.2935 - accuracy: 0.8923\n",
            "Epoch 26: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.2636 - accuracy: 0.9171 - val_loss: 0.1294 - val_accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "33/41 [=======================>......] - ETA: 0s - loss: 0.2281 - accuracy: 0.9394\n",
            "Epoch 27: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2321 - accuracy: 0.9366 - val_loss: 0.1214 - val_accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "29/41 [====================>.........] - ETA: 0s - loss: 0.2318 - accuracy: 0.9448\n",
            "Epoch 28: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.2236 - accuracy: 0.9415 - val_loss: 0.1151 - val_accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "26/41 [==================>...........] - ETA: 0s - loss: 0.2777 - accuracy: 0.9308\n",
            "Epoch 29: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.2499 - accuracy: 0.9366 - val_loss: 0.1143 - val_accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "28/41 [===================>..........] - ETA: 0s - loss: 0.2061 - accuracy: 0.9357\n",
            "Epoch 30: saving model to keypoint_classifier.hdf5\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.2282 - accuracy: 0.9366 - val_loss: 0.1129 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f22ebc0b3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**shows us the final accuracy of the model**"
      ],
      "metadata": {
        "id": "HMZ5sWBcYPIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if Detailed_results:\n",
        "  val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=64\n",
        "                                    )\n",
        "  # Loading the saved model\n",
        "  model = tf.keras.models.load_model(model_save_path)\n",
        "\n",
        "# Inference test\n",
        "# predict_result = model.predict(np.array([X_test[0]]))\n",
        "# print(np.squeeze(predict_result))\n",
        "# print(np.argmax(np.squeeze(predict_result)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjS3wVnQYPXV",
        "outputId": "76d334c0-176f-4e3a-f694-fe22f45f193a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1129 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONFUSION MATRIX**(grahical and tabular representation of loss & accuracy)"
      ],
      "metadata": {
        "id": "j6Z4ghXnY0ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def print_confusion_matrix(y_true, y_pred, report=True):\n",
        "    labels = sorted(list(set(y_true)))\n",
        "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
        "    ax.set_ylim(len(set(y_true)), 0)\n",
        "    plt.show()\n",
        "\n",
        "    if report:\n",
        "        print('Classification Report')\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "if Detailed_results:\n",
        "  Y_pred = model.predict(X_test)\n",
        "  y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "  print_confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "l7a8mjvWY0tV",
        "outputId": "3160c034-2ee4-4ad9-ac07-db195e978789"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        34\n",
            "           1       1.00      1.00      1.00        35\n",
            "\n",
            "    accuracy                           1.00        69\n",
            "   macro avg       1.00      1.00      1.00        69\n",
            "weighted avg       1.00      1.00      1.00        69\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**saves the model as .tflite file in the specified path**"
      ],
      "metadata": {
        "id": "0Qg2L4iEZaN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(model_save_path, include_optimizer=False)\n",
        "[35]\n",
        "# Transform model (quantization)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quantized_model = converter.convert()\n",
        "\n",
        "with open(tflite_save_path, 'wb') as f:\n",
        "    f.write(tflite_quantized_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAHgkYJbZaZR",
        "outputId": "872a9e63-59ca-4f61-86ff-307eaf52f5a9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE TEST**"
      ],
      "metadata": {
        "id": "QN7kuWLWqSMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if Detailed_results:\n",
        "  interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Get I / O tensor\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "\n",
        "  interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]]))\n",
        "\n",
        "\n",
        "  # Inference implementation\n",
        "  interpreter.invoke()\n",
        "  tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "\n",
        "  print(np.squeeze(tflite_results))\n",
        "  print(np.argmax(np.squeeze(tflite_results)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqitD01PqV64",
        "outputId": "54d1b967-c909-433d-8e05-a21e8878ce66"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.99075294 0.00924709]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EFJfKS0nZ1cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip compressed.zip\n",
        "\n",
        "# !zip -rq9 archive.zip mohith\n",
        "\n",
        "# !Compress-Archive -Path 'directory' -DestinationPath 'archive.zip'\n",
        "\n",
        "#labels should b written in lexographical order!\n"
      ],
      "metadata": {
        "id": "Ql1cGtrsZ0px"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}